# Attention
- What does Attention in Neural Machine Translation Pay Attention to? [paper](https://arxiv.org/pdf/1710.03348.pdf)  
Some interesting findings, and more details in the paper:  
~ only 54% attention to alignment points, which NUM 73%, NOUN 68%, VERB just 49%, and PRT(Particle, such as â€™s, off, up) just 36%.  
~ Attention acc on alignments are high for NOUN, and very low for the VERB, but their target losses are about the same.
