# NMT
- Convolutional Sequence to Sequence Learning [paper](https://arxiv.org/abs/1705.03122)/[offical code](https://github.com/facebookresearch/fairseq-py)  

- Attention Is All You Need [paper](https://arxiv.org/abs/1706.03762)/[offical code](https://github.com/tensorflow/tensor2tensor)/[pytorch code](https://github.com/jadore801120/attention-is-all-you-need-pytorch)  
To learn more about self-attention mechanism, you could read "[A Structured Self-attentive Sentence Embedding](https://arxiv.org/pdf/1703.03130.pdf)".